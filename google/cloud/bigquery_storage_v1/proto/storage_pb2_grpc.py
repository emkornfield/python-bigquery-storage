# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
import grpc
import traceback 

from google.protobuf.internal.wire_format import (WIRETYPE_LENGTH_DELIMITED, WIRETYPE_VARINT, WIRETYPE_FIXED64, WIRETYPE_FIXED32, UnpackTag)
from google.protobuf.internal.decoder import _DecodeVarint, _DecodeVarint32
from collections import namedtuple

from google.cloud.bigquery_storage_v1.proto import (
    storage_pb2 as google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_storage__pb2,
)
from google.cloud.bigquery_storage_v1.proto import (
    stream_pb2 as google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_stream__pb2,
)

class CustomReadRowsResponseParse(object):
    arrow_batch = namedtuple('ArrowRecordBatch',  ['row_count', 'serialized_record_batch'])
    row_response = namedtuple('ReadRowsResponse', ['row_count', 'arrow_record_batch'])
    @classmethod
    def SkipField(cls, serialized, idx):
         tag, idx = _DecodeVarint32(serialized, idx)
         field, wire_type = UnpackTag(tag)
         if wire_type == WIRETYPE_LENGTH_DELIMITED:
             length, idx = _DecodeVarint32(serialized, idx)
             return idx + length
         if wire_type == WIRETYPE_VARINT:
            _, idx = _DecodeVarint32(serialized, idx)
            return idx
         if wire_type == WIRETYPE_FIXED64:
             return idx + 8
         if wire_type == WIRETYPE_FIXED32:
             return idx + 4
         raise InvalidError("Unsupported value tag={tag} wire_type={wire_type}")

    @classmethod
    def ParseArrowStruct(cls, serialized, idx, struct_end):
        batch = ''
        SERIALIZED_RECORD_BATCH_TAG= (1 << 3 | WIRETYPE_LENGTH_DELIMITED)
        while idx < struct_end:
            if serialized[idx] == SERIALIZED_RECORD_BATCH_TAG:
                idx += 1
                arrow_batch_len, idx = _DecodeVarint32(serialized, idx)
                batch = memoryview(serialized)[idx:idx+arrow_batch_len]
                idx += arrow_batch_len
            else:
                idx = cls.SkipField(serialized, idx)
        return batch

    @classmethod
    def FromString(cls, serialized):
        idx = 0
        ARROW_FIELD_TAG = (4 << 3 | WIRETYPE_LENGTH_DELIMITED)
        ROW_COUNT_TAG = (6 << 3 | WIRETYPE_VARINT)
        row_count = 0
        batch = b''
        while idx < len(serialized) and not (batch and row_count):
            if serialized[idx] == ARROW_FIELD_TAG:
                idx += 1
                arrow_struct_len, idx = _DecodeVarint32(serialized, idx)
                batch = cls.ParseArrowStruct(serialized, idx, idx + arrow_struct_len)
            elif serialized[idx] == ROW_COUNT_TAG:
                idx += 1
                row_count, idx = _DecodeVarint(serialized, idx) 
            else:
                idx = cls.SkipField(serialized, idx)
        return cls.row_response(row_count, cls.arrow_batch(row_count, batch))
                  

class BigQueryReadStub(object):
    """BigQuery Read API.

  The Read API can be used to read data from BigQuery.
  """

    def __init__(self, channel):
        """Constructor.

    Args:
      channel: A grpc.Channel.
    """
        self.CreateReadSession = channel.unary_unary(
            "/google.cloud.bigquery.storage.v1.BigQueryRead/CreateReadSession",
            request_serializer=google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_storage__pb2.CreateReadSessionRequest.SerializeToString,
            response_deserializer=google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_stream__pb2.ReadSession.FromString,
        )
        self.ReadRows = channel.unary_stream(
            "/google.cloud.bigquery.storage.v1.BigQueryRead/ReadRows",
            request_serializer=google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_storage__pb2.ReadRowsRequest.SerializeToString,
            response_deserializer=CustomReadRowsResponseParse.FromString,
        )
        self.SplitReadStream = channel.unary_unary(
            "/google.cloud.bigquery.storage.v1.BigQueryRead/SplitReadStream",
            request_serializer=google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_storage__pb2.SplitReadStreamRequest.SerializeToString,
            response_deserializer=google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_storage__pb2.SplitReadStreamResponse.FromString,
        )


class BigQueryReadServicer(object):
    """BigQuery Read API.

  The Read API can be used to read data from BigQuery.
  """

    def CreateReadSession(self, request, context):
        """Creates a new read session. A read session divides the contents of a
    BigQuery table into one or more streams, which can then be used to read
    data from the table. The read session also specifies properties of the
    data to be read, such as a list of columns or a push-down filter describing
    the rows to be returned.

    A particular row can be read by at most one stream. When the caller has
    reached the end of each stream in the session, then all the data in the
    table has been read.

    Data is assigned to each stream such that roughly the same number of
    rows can be read from each stream. Because the server-side unit for
    assigning data is collections of rows, the API does not guarantee that
    each stream will return the same number or rows. Additionally, the
    limits are enforced based on the number of pre-filtered rows, so some
    filters can lead to lopsided assignments.

    Read sessions automatically expire 24 hours after they are created and do
    not require manual clean-up by the caller.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def ReadRows(self, request, context):
        """Reads rows from the stream in the format prescribed by the ReadSession.
    Each response contains one or more table rows, up to a maximum of 100 MiB
    per response; read requests which attempt to read individual rows larger
    than 100 MiB will fail.

    Each request also returns a set of stream statistics reflecting the current
    state of the stream.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def SplitReadStream(self, request, context):
        """Splits a given `ReadStream` into two `ReadStream` objects. These
    `ReadStream` objects are referred to as the primary and the residual
    streams of the split. The original `ReadStream` can still be read from in
    the same manner as before. Both of the returned `ReadStream` objects can
    also be read from, and the rows returned by both child streams will be
    the same as the rows read from the original stream.

    Moreover, the two child streams will be allocated back-to-back in the
    original `ReadStream`. Concretely, it is guaranteed that for streams
    original, primary, and residual, that original[0-j] = primary[0-j] and
    original[j-n] = residual[0-m] once the streams have been read to
    completion.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")


def add_BigQueryReadServicer_to_server(servicer, server):
    rpc_method_handlers = {
        "CreateReadSession": grpc.unary_unary_rpc_method_handler(
            servicer.CreateReadSession,
            request_deserializer=google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_storage__pb2.CreateReadSessionRequest.FromString,
            response_serializer=google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_stream__pb2.ReadSession.SerializeToString,
        ),
        "ReadRows": grpc.unary_stream_rpc_method_handler(
            servicer.ReadRows,
            request_deserializer=google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_storage__pb2.ReadRowsRequest.FromString,
            response_serializer=google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_storage__pb2.ReadRowsResponse.SerializeToString,
        ),
        "SplitReadStream": grpc.unary_unary_rpc_method_handler(
            servicer.SplitReadStream,
            request_deserializer=google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_storage__pb2.SplitReadStreamRequest.FromString,
            response_serializer=google_dot_cloud_dot_bigquery__storage__v1_dot_proto_dot_storage__pb2.SplitReadStreamResponse.SerializeToString,
        ),
    }
    generic_handler = grpc.method_handlers_generic_handler(
        "google.cloud.bigquery.storage.v1.BigQueryRead", rpc_method_handlers
    )
    server.add_generic_rpc_handlers((generic_handler,))
